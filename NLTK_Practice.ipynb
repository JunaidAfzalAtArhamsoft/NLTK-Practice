{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK Practice.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "y_EwqZvpguNG"
      ],
      "authorship_tag": "ABX9TyP4JuUYzyokOFBohr0PogCp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunaidAfzalAtArhamsoft/NLTK-Practice/blob/main/NLTK_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y29lpp4g4km"
      },
      "source": [
        "# NLTK Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGxm3r5sEZB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5B0pYTflUf_"
      },
      "source": [
        "** Install below tools to work properly **\n",
        "\n",
        "Only one time setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdW4sndIi5rK",
        "outputId": "44a99c68-5662-4611-bf5c-ac74d184c03f"
      },
      "source": [
        "# Run first time to install it, so that nltk can worj correctly\n",
        "nltk.download('punkt') "
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0PHkEcQsCwu",
        "outputId": "d1f0d985-4baa-4ab0-87b7-ddad5b85c262"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PisoHu3p229H",
        "outputId": "691b80c5-40c6-4047-da6e-d20c15bf7dee"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLaiSey69fAK",
        "outputId": "d3013665-3b99-423b-d206-df17dcfb1f7c"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtHxZm6G9k5L",
        "outputId": "c63be36a-9450-4e5f-822b-72d246686bbb"
      },
      "source": [
        "nltk.download('words')"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_EwqZvpguNG"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfaESY8XhY5-"
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvXuLMdNhexy"
      },
      "source": [
        "sentence = 'I live in Pakistan and I love my country. Pakistan is very beautiful country. The capital of Pakistan is Islamabad. But I live in Lahore.'"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-6Va8_-he3o"
      },
      "source": [
        "sentence_tokenize = word_tokenize(sentence)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chfEnfhNjNfO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHms3h-Nhe7K",
        "outputId": "38ff7a52-1a4f-4dea-bc0d-cb02a9276b6d"
      },
      "source": [
        "sentence_tokenize"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'live',\n",
              " 'in',\n",
              " 'Pakistan',\n",
              " 'and',\n",
              " 'I',\n",
              " 'love',\n",
              " 'my',\n",
              " 'country',\n",
              " '.',\n",
              " 'Pakistan',\n",
              " 'is',\n",
              " 'very',\n",
              " 'beautiful',\n",
              " 'country',\n",
              " '.',\n",
              " 'The',\n",
              " 'capital',\n",
              " 'of',\n",
              " 'Pakistan',\n",
              " 'is',\n",
              " 'Islamabad',\n",
              " '.',\n",
              " 'But',\n",
              " 'I',\n",
              " 'live',\n",
              " 'in',\n",
              " 'Lahore',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s6F-F7Yjnib"
      },
      "source": [
        "## Finding Probability and common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcyl-ti0jMM8"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "freq_obj = FreqDist()"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPYgmyLxj3uH"
      },
      "source": [
        "for token in sentence_tokenize:\n",
        "  freq_obj[token] = freq_obj[token] + 1"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF__wdQqkcSu",
        "outputId": "f9d5395b-4f18-4e76-9558-19d029e800e0"
      },
      "source": [
        "freq_obj"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'.': 4,\n",
              "          'But': 1,\n",
              "          'I': 3,\n",
              "          'Islamabad': 1,\n",
              "          'Lahore': 1,\n",
              "          'Pakistan': 3,\n",
              "          'The': 1,\n",
              "          'and': 1,\n",
              "          'beautiful': 1,\n",
              "          'capital': 1,\n",
              "          'country': 2,\n",
              "          'in': 2,\n",
              "          'is': 2,\n",
              "          'live': 2,\n",
              "          'love': 1,\n",
              "          'my': 1,\n",
              "          'of': 1,\n",
              "          'very': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ9YTZfrlr25"
      },
      "source": [
        "**Finding Common Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R0yJEW_ln_a",
        "outputId": "793a00fc-abe2-401f-b48f-8cbeea84a505"
      },
      "source": [
        " # Finding Top 4 most common words.\n",
        " top_4 = freq_obj.most_common(4)\n",
        " top_4"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 4), ('I', 3), ('Pakistan', 3), ('live', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYb_yCOemfNO"
      },
      "source": [
        "## Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12U_QmwLloWs"
      },
      "source": [
        "sentence = 'My name is Junaid Afzal. I live in Lahore and I work in Arhamsoft as a Python Developer'"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZT5BNhgloZx"
      },
      "source": [
        "sentence_token = word_tokenize(sentence)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J41rDZeDlohS"
      },
      "source": [
        "sentence_bigrams = list(nltk.bigrams(sentence_token))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEjtD1t_nYCY",
        "outputId": "5ea1804e-cc17-4822-f931-e210599b4425"
      },
      "source": [
        "sentence_bigrams"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('My', 'name'),\n",
              " ('name', 'is'),\n",
              " ('is', 'Junaid'),\n",
              " ('Junaid', 'Afzal'),\n",
              " ('Afzal', '.'),\n",
              " ('.', 'I'),\n",
              " ('I', 'live'),\n",
              " ('live', 'in'),\n",
              " ('in', 'Lahore'),\n",
              " ('Lahore', 'and'),\n",
              " ('and', 'I'),\n",
              " ('I', 'work'),\n",
              " ('work', 'in'),\n",
              " ('in', 'Arhamsoft'),\n",
              " ('Arhamsoft', 'as'),\n",
              " ('as', 'a'),\n",
              " ('a', 'Python'),\n",
              " ('Python', 'Developer')]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuqVmE96njhD"
      },
      "source": [
        "## Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGRStd1RnYGt"
      },
      "source": [
        "sentence_trigrams = list(nltk.trigrams(sentence_token))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH5YuAJknYSm",
        "outputId": "e17d599f-02e7-440a-9f9c-bd5187cf2a46"
      },
      "source": [
        "sentence_trigrams"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('My', 'name', 'is'),\n",
              " ('name', 'is', 'Junaid'),\n",
              " ('is', 'Junaid', 'Afzal'),\n",
              " ('Junaid', 'Afzal', '.'),\n",
              " ('Afzal', '.', 'I'),\n",
              " ('.', 'I', 'live'),\n",
              " ('I', 'live', 'in'),\n",
              " ('live', 'in', 'Lahore'),\n",
              " ('in', 'Lahore', 'and'),\n",
              " ('Lahore', 'and', 'I'),\n",
              " ('and', 'I', 'work'),\n",
              " ('I', 'work', 'in'),\n",
              " ('work', 'in', 'Arhamsoft'),\n",
              " ('in', 'Arhamsoft', 'as'),\n",
              " ('Arhamsoft', 'as', 'a'),\n",
              " ('as', 'a', 'Python'),\n",
              " ('a', 'Python', 'Developer')]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r6ngvRjou26"
      },
      "source": [
        "## N-Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-S8vfozpBHD"
      },
      "source": [
        "sentence = 'My name is Junaid Afzal. I live in Lahore and I work in Arhamsoft as a Python Developer'"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD5d3AEcpCvG"
      },
      "source": [
        "sentence_token = word_tokenize(sentence)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uHGHW35pCsl"
      },
      "source": [
        "# Making n-grams of 4 words.\n",
        "n_grams = list(nltk.ngrams(sentence_token, 4))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYQqJ6wJpCxs",
        "outputId": "1a00d624-4073-494f-a113-ba6a088b150c"
      },
      "source": [
        "n_grams"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('My', 'name', 'is', 'Junaid'),\n",
              " ('name', 'is', 'Junaid', 'Afzal'),\n",
              " ('is', 'Junaid', 'Afzal', '.'),\n",
              " ('Junaid', 'Afzal', '.', 'I'),\n",
              " ('Afzal', '.', 'I', 'live'),\n",
              " ('.', 'I', 'live', 'in'),\n",
              " ('I', 'live', 'in', 'Lahore'),\n",
              " ('live', 'in', 'Lahore', 'and'),\n",
              " ('in', 'Lahore', 'and', 'I'),\n",
              " ('Lahore', 'and', 'I', 'work'),\n",
              " ('and', 'I', 'work', 'in'),\n",
              " ('I', 'work', 'in', 'Arhamsoft'),\n",
              " ('work', 'in', 'Arhamsoft', 'as'),\n",
              " ('in', 'Arhamsoft', 'as', 'a'),\n",
              " ('Arhamsoft', 'as', 'a', 'Python'),\n",
              " ('as', 'a', 'Python', 'Developer')]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YygyM740pAjf"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmUrHIYhviXD"
      },
      "source": [
        "**Defination**: \n",
        "\n",
        "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n",
        "\n",
        "Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as **(-ed,-ize, -s,-de,mis)**. So stemming a word or sentence may **result in words that are not actual words**. **Stems are created by removing the suffixes or prefixes used with a word**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSW2HA2GpC0X"
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltP6vdYsp_ST"
      },
      "source": [
        "porter_stemmer = PorterStemmer()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GVKkuYMs1P8"
      },
      "source": [
        "words_to_stem = ['winning', 'sing', 'singing', 'sleeps', 'worked', 'studies', 'booked', 'Giving', 'cacti', 'am', 'are', 'is']"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqRtf05esqYQ",
        "outputId": "88a1453d-9389-46ea-ad31-21826f964b80"
      },
      "source": [
        "print('*word*\\t\\t*Stem*\\n')\n",
        "for word in words_to_stem:\n",
        "  print(word + '\\t-->\\t' + porter_stemmer.stem(word))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*word*\t\t*Stem*\n",
            "\n",
            "winning\t-->\twin\n",
            "sing\t-->\tsing\n",
            "singing\t-->\tsing\n",
            "sleeps\t-->\tsleep\n",
            "worked\t-->\twork\n",
            "studies\t-->\tstudi\n",
            "booked\t-->\tbook\n",
            "Giving\t-->\tgive\n",
            "cacti\t-->\tcacti\n",
            "am\t-->\tam\n",
            "are\t-->\tare\n",
            "is\t-->\tis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M60BdH6SrU2U"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psP7hHMHv-FN"
      },
      "source": [
        "Defination:\n",
        "\n",
        "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
        "\n",
        "Python NLTK provides WordNet Lemmatizer that uses the WordNet Database to lookup lemmas of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c9iAarhrYQm"
      },
      "source": [
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjdxY2NquXpJ"
      },
      "source": [
        "words_to_lemmatize = ['studies', 'geese', 'cacti', 'am', 'are', 'is']"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGwofIwRrYXN"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y976H4tSrYmP",
        "outputId": "cded4666-44c8-468e-d740-45213558c9d9"
      },
      "source": [
        "print('*word*\\t\\t*Lemma*\\n')\n",
        "for word in words_to_lemmatize:\n",
        "  print(word + '\\t-->\\t' + lemmatizer.lemmatize(word))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*word*\t\t*Lemma*\n",
            "\n",
            "studies\t-->\tstudy\n",
            "geese\t-->\tgoose\n",
            "cacti\t-->\tcactus\n",
            "am\t-->\tam\n",
            "are\t-->\tare\n",
            "is\t-->\tis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSQRypAt1-TZ"
      },
      "source": [
        "## Parts of Speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG8gMrCA5r9V"
      },
      "source": [
        "**About**\n",
        "\n",
        "The POS tagger in the NLTK library outputs specific tags for certain words. The list of POS tags is as follows, with examples of what each POS stands for.\n",
        "\n",
        "\n",
        "\n",
        "The POS tagger in the NLTK library outputs specific tags for certain words. The list of POS tags is as follows, with examples of what each POS stands for.\n",
        "\n",
        "* CC coordinating conjunction\n",
        "\n",
        "* CD cardinal digit\n",
        "\n",
        "* DT determiner\n",
        "\n",
        "* EX existential there (like: “there is” … think of it like “there exists”)\n",
        "\n",
        "* FW foreign word\n",
        "\n",
        "* IN preposition/subordinating conjunction\n",
        "\n",
        "* JJ adjective ‘big’\n",
        "\n",
        "* JJR adjective, comparative ‘bigger’\n",
        "\n",
        "* JJS adjective, superlative ‘biggest’\n",
        "\n",
        "* LS list marker 1)\n",
        "\n",
        "* MD modal could, will\n",
        "\n",
        "* NN noun, singular ‘desk’\n",
        "\n",
        "* NNS noun plural ‘desks’\n",
        "\n",
        "* NNP proper noun, singular ‘Harrison’\n",
        "\n",
        "* NNPS proper noun, plural ‘Americans’\n",
        "\n",
        "* PDT predeterminer ‘all the kids’\n",
        "\n",
        "* POS possessive ending parent’s\n",
        "\n",
        "* PRP personal pronoun I, he, she\n",
        "\n",
        "* PRP$ possessive pronoun my, his, hers\n",
        "\n",
        "* RB adverb very, silently,\n",
        "\n",
        "* RBR adverb, comparative better\n",
        "\n",
        "* RBS adverb, superlative best\n",
        "\n",
        "* RP particle give up\n",
        "\n",
        "* TO, to go ‘to’ the store.\n",
        "\n",
        "* UH interjection, errrrrrrrm\n",
        "\n",
        "* VB verb, base form take\n",
        "\n",
        "* VBD verb, past tense took\n",
        "\n",
        "* VBG verb, gerund/present participle taking\n",
        "\n",
        "* VBN verb, past participle taken\n",
        "\n",
        "* VBP verb, sing. present, non-3d take\n",
        "\n",
        "* VBZ verb, 3rd person sing. present takes\n",
        "\n",
        "* WDT wh-determiner which\n",
        "\n",
        "* WP wh-pronoun who, what\n",
        "\n",
        "* WP$ possessive wh-pronoun whose\n",
        "\n",
        "* WRB wh-abverb where, when\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0DAL6g118-8"
      },
      "source": [
        "sentence = 'Junaid work in Arhamsoft and live in Pakistan.'"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb6L7YAC19kq"
      },
      "source": [
        "sentence_token = word_tokenize(sentence)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDxWio4t3QC6",
        "outputId": "8f64f9dd-6dcb-4c2e-86c6-f2d5bf437bbf"
      },
      "source": [
        "sentence_token"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Junaid', 'work', 'in', 'Arhamsoft', 'and', 'live', 'in', 'Pakistan', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5YBCbIF19nk",
        "outputId": "2f3b4a15-7019-428e-d9d6-7bf5b1d7411a"
      },
      "source": [
        "for word in sentence_token:\n",
        "  print(nltk.pos_tag([word]))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Junaid', 'NN')]\n",
            "[('work', 'NN')]\n",
            "[('in', 'IN')]\n",
            "[('Arhamsoft', 'NNP')]\n",
            "[('and', 'CC')]\n",
            "[('live', 'JJ')]\n",
            "[('in', 'IN')]\n",
            "[('Pakistan', 'NN')]\n",
            "[('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1woFM1j28Ac7"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8IzlZ108Mn7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOxfBZlv7bj4"
      },
      "source": [
        "from nltk import ne_chunk"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzpQIKoy7bmf"
      },
      "source": [
        "sentence = \"Arhamsoft earn many certificates. Arhamsoft is hiring fresh graguates. CEO of Arhamsoft is Muhammad Irfan Zafar.\""
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqOAxXrf7bpS"
      },
      "source": [
        "sentence_token = word_tokenize(sentence)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtiF2ICy7bsa"
      },
      "source": [
        "sentence_tags = nltk.pos_tag(sentence_token)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnlpYw0H9P5y"
      },
      "source": [
        "named_entity = ne_chunk(sentence_tags)"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drfzR5Pt9P8O",
        "outputId": "90404f4b-b027-42f4-c7b3-ede19d88699a"
      },
      "source": [
        "print(named_entity)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Arhamsoft/NNP)\n",
            "  earn/VBP\n",
            "  many/JJ\n",
            "  certificates/NNS\n",
            "  ./.\n",
            "  (PERSON Arhamsoft/NNP)\n",
            "  is/VBZ\n",
            "  hiring/VBG\n",
            "  fresh/JJ\n",
            "  graguates/NNS\n",
            "  ./.\n",
            "  CEO/NN\n",
            "  of/IN\n",
            "  (ORGANIZATION Arhamsoft/NNP)\n",
            "  is/VBZ\n",
            "  (PERSON Muhammad/NNP Irfan/NNP Zafar/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HMZHyadBonY"
      },
      "source": [
        "# Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0U4VSyYBlu-"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfA6WC9sFtAz"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "codC0XWnGPeK"
      },
      "source": [
        "sentence = 'I am going to watch movie with my friends. Bilal and Shah is also with me.'"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDPbkb-5FtDD"
      },
      "source": [
        "sentence_token = nlp(sentence)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHDfE9rxGgYk",
        "outputId": "f0a4f815-1b5c-4335-c57e-26a1cb2c1563"
      },
      "source": [
        "type(sentence_token)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka9C39W7HEkB",
        "outputId": "da886fa0-fee6-4e01-be8f-e25b38cda6af"
      },
      "source": [
        "sentence_token"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I am going to watch movie with my friends. Bilal and Shah is also with me."
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL6ea4bNFtFi",
        "outputId": "f246b7cd-c0bd-4988-f213-b27f0e11e013"
      },
      "source": [
        "# Printing token with its index\n",
        "for token in sentence_token:\n",
        "  print(f'{token.i}\\t{token.text}') "
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tI\n",
            "1\tam\n",
            "2\tgoing\n",
            "3\tto\n",
            "4\twatch\n",
            "5\tmovie\n",
            "6\twith\n",
            "7\tmy\n",
            "8\tfriends\n",
            "9\t.\n",
            "10\tBilal\n",
            "11\tand\n",
            "12\tShah\n",
            "13\tis\n",
            "14\talso\n",
            "15\twith\n",
            "16\tme\n",
            "17\t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-BSg6qqF-wu",
        "outputId": "f553b17b-0a5d-4874-c8f5-577c9f442942"
      },
      "source": [
        "sentence_token[0:5]"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I am going to watch"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7G4ZRnIPm1g",
        "outputId": "e62ab8d9-377f-4cf6-ab23-2c4b641528d6"
      },
      "source": [
        "# Printing tokens with pos_tags\n",
        "for token in sentence_token:\n",
        "  print(f'{token.i}\\t{token.text}\\t{token.pos_}') "
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tI\tPRON\n",
            "1\tam\tAUX\n",
            "2\tgoing\tVERB\n",
            "3\tto\tPART\n",
            "4\twatch\tVERB\n",
            "5\tmovie\tNOUN\n",
            "6\twith\tADP\n",
            "7\tmy\tDET\n",
            "8\tfriends\tNOUN\n",
            "9\t.\tPUNCT\n",
            "10\tBilal\tPROPN\n",
            "11\tand\tCCONJ\n",
            "12\tShah\tPROPN\n",
            "13\tis\tAUX\n",
            "14\talso\tADV\n",
            "15\twith\tADP\n",
            "16\tme\tPRON\n",
            "17\t.\tPUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojvlbJWQPm4L",
        "outputId": "14618555-71df-4c30-de90-a391429ac422"
      },
      "source": [
        "# Printint with token label\n",
        "for token in sentence_token.ents:\n",
        "  print(f'{token.text}\\t{token.label_}') "
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shah\tORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC7-1Zd9Pm7E"
      },
      "source": [
        "new_sentence = 'I am going to watch movie with my friends. John Wick and Albert is also with me. Albert work in Arhamsoft.'"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pJbwtBORis0"
      },
      "source": [
        "new_sentence_doc = nlp(new_sentence)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joWhrXUVPnAU",
        "outputId": "fcc96266-8aa7-4eb9-cdb4-843f711f9e81"
      },
      "source": [
        " # Printint with token label\n",
        "for token in new_sentence_doc.ents:\n",
        "  print(f'{token.text}\\t{token.label_}') "
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John Wick\tPERSON\n",
            "Albert\tPERSON\n",
            "Albert\tPERSON\n",
            "Arhamsoft\tORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu3kNCoKSdxI"
      },
      "source": [
        "urdu_sentence = 'Junaid Arhamsoft ma kaam karta ha.'"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku2XGTKhSdzn"
      },
      "source": [
        "urdu_doc = nlp(urdu_sentence)"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT7KP-75Sd19",
        "outputId": "009ce57f-5c0f-444a-896d-90fa02805c50"
      },
      "source": [
        " # Printing urdu_sentence result with token label\n",
        "for token in urdu_doc.ents:\n",
        "  print(f'{token.text}\\t{token.label_}') "
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Junaid Arhamsoft\tPERSON\n",
            "kaam karta\tPERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkGd4uQjSd4h"
      },
      "source": [
        "chinese_sentence = '我在 Arhamsoft 工作' # I work in Arhamsoft"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XE1L7kmSePC"
      },
      "source": [
        "chinese_doc = nlp(chinese_sentence)"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cG1H9WZSeYL"
      },
      "source": [
        " # Printing chinese_sentence result with token label\n",
        "for token in chinese_doc.ents:\n",
        "  print(f'{token.text}\\t{token.label_}') "
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdyUkCJATZz6"
      },
      "source": [
        "eng_sentence = 'I work in Arhamsoft'\n",
        "eng_doc = nlp(eng_sentence)"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRyvkY9xTp1l",
        "outputId": "f5437f63-63d7-4aad-e5a2-c97f1ea62e81"
      },
      "source": [
        " # Printing eng_doc result with token label\n",
        "for token in eng_doc.ents:\n",
        "  print(f'{token.text}\\t{token.label_}') "
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arhamsoft\tORG\n"
          ]
        }
      ]
    }
  ]
}